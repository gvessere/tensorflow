{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Execution: Working with Gradients\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "* How to get gradients using TensorFlow's eager execution capabilities\n",
    "* How to apply the gradients so you can update your variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: Import eager and enable eager execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import TensorFlow eager execution support (subject to future changes).\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "# Enable eager execution.\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Simple Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Synthesize some data\n",
    "\n",
    "To demonstrate fitting a model with TensorFlow's eager execution, we'll fit a linear model to some synthesized data (which includes some noise).\n",
    "\n",
    "In the code, we  use the variable names `w` and `b` to represent the single weight and bias we'll use to fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The constants we'll try to fit our variables to:\n",
    "true_w = 3\n",
    "true_b = 2\n",
    "\n",
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "# Our inputs:\n",
    "inputs = tf.random_normal(shape=[NUM_EXAMPLES, 1])\n",
    "\n",
    "# Our labels, with noise:\n",
    "noise = tf.random_normal(shape=[NUM_EXAMPLES, 1])\n",
    "labels = inputs * true_w + true_b + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data (Optional)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(inputs.numpy(), labels.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define our TensorFlow variables\n",
    "\n",
    "We'll use Keras's object-oriented [`Dense`](https://www.tensorflow.org/api_docs/python/tf/contrib/keras/layers/Dense) layer to create our variables. In this case, we'll create a `Dense` layer with a single weight and bias.\n",
    "\n",
    "(**Note**: We're using the implementation of `Dense` found in `tf.layers.Dense` though the documentation link is for `tf.contrib.keras.layers.Dense`. When TensorFlow 1.4 is released, the documentation will also be in `tf.layers.Dense`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow Variables using Keras's Dense layer.\n",
    "\n",
    "wb = tf.layers.Dense(units=1, use_bias=True)\n",
    "\n",
    "# We can access the underlying TensorFlow variables using wb.variables.\n",
    "# However, the variables won't exist until the dimensions of the input\n",
    "# tensors are known. Once the dimensions of the input tensors are known,\n",
    "# Keras can create and initialize the variables. Until then, Keras will\n",
    "# report the variables as an empty list: [].\n",
    "\n",
    "wb.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define our loss function\n",
    "\n",
    "Our loss function is the standard L2 loss (where we reduce the loss to its mean across its inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(inputs, labels, wb):\n",
    "  \"\"\"Calculates the mean L2 loss for our linear model.\"\"\"\n",
    "  predictions = wb(inputs)\n",
    "  return tf.reduce_mean(tf.square(predictions - labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss function (optional).\n",
    "\n",
    "loss_fn(inputs, labels, wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, the variables exist, and can now be queried:\n",
    "\n",
    "w, b = wb.variables\n",
    "print(\"w: \" + str(w.read_value()))\n",
    "print(\"b: \" + str(b.read_value()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create our gradients function using `implicit_value_and_gradients()`\n",
    "\n",
    "With a loss function defined, we can calculate gradients and apply them to our variables to update them.\n",
    "\n",
    "To calculate the gradients, we wrap our loss function using the `implicit_value_and_gradients()` function.\n",
    "\n",
    "`implicit_value_and_gradients()` returns a function that accepts the same inputs as the function passed in, and returns a tuple consisting of:\n",
    "\n",
    "1. the value returned by the function passed in (in this case, the loss calculated by `loss_fn()`), and\n",
    "1. a list of tuples consisting of:\n",
    "  1. The value of the gradient (a `tf.Tensor`) with respect to a given variable\n",
    "  1. The corresponding variable (`tf.Variable`)\n",
    "\n",
    "Test it out below to get a feel for what it does. Notice how the first value of the returned tuple (the loss) is the same as the value returned in the cell above that tests our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce our gradients function. See description above for details about\n",
    "# the returned function's signature.\n",
    "\n",
    "value_and_gradients_fn = tfe.implicit_value_and_gradients(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show outputs of value_and_gradients_fn.\n",
    "\n",
    "print(\"Outputs of value_and_gradients_fn:\")\n",
    "\n",
    "value, grads_and_vars = value_and_gradients_fn(inputs, labels, wb)\n",
    "\n",
    "print('Loss: {}'.format(value))\n",
    "for (grad, var) in grads_and_vars:\n",
    "  print(\"\")\n",
    "  print('Gradient: {}\\nVariable: {}'.format(grad, var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create an optimizer\n",
    "\n",
    "We'll use a `GradientDescentOptimizer` to fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5a: Test Our Optimizer\n",
    "\n",
    "Now we have everything needed to start fitting our variables to the data!\n",
    "\n",
    "In the next cell, we'll demo these capabilities. We'll:\n",
    "\n",
    "1. Print the current values of `w` and `b`\n",
    "1. Calculate the loss and gradients\n",
    "1. Apply the gradients\n",
    "1. Print out the new values of `w` and `b`\n",
    "\n",
    "You can run the cell multiple times. Each time, you should see the values of `w` and `b` get closer to their true values of 3 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the optimizer.\n",
    "\n",
    "print(\"Values of w, b, BEFORE applying gradients:\")\n",
    "w, b = wb.variables\n",
    "print(w.read_value().numpy(), b.read_value().numpy())\n",
    "print()\n",
    "\n",
    "# Calculate the gradients:\n",
    "empirical_loss, gradients_and_variables = value_and_gradients_fn(\n",
    "    inputs, labels, wb)\n",
    "optimizer.apply_gradients(gradients_and_variables)\n",
    "\n",
    "print(\"Values of w, b, AFTER applying gradients:\")\n",
    "print(w.read_value().numpy(), b.read_value().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create a training loop\n",
    "\n",
    "Of course, now we can simply turn all of this code into a self-standing training loop. We'll also capture our loss and approximations of `w` and `b` and plot them over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our variables.\n",
    "\n",
    "# numpy is used for its asscalar() function.\n",
    "import numpy as np\n",
    "\n",
    "num_training_steps = 10\n",
    "\n",
    "def train_model(inputs, labels, wb, optimizer, num_training_steps):\n",
    "  loss_at_step = []\n",
    "  w_at_step = []\n",
    "  b_at_step = []\n",
    "  for step_num in range(num_training_steps):\n",
    "    loss, gradients_and_variables = value_and_gradients_fn(inputs, labels, wb)\n",
    "    loss_at_step.append(np.asscalar(loss.numpy()))\n",
    "    \n",
    "    optimizer.apply_gradients(gradients_and_variables)\n",
    "    w, b = wb.variables\n",
    "    w_at_step.append(np.asscalar(w.read_value().numpy()))\n",
    "    b_at_step.append(np.asscalar(b.read_value().numpy()))\n",
    "\n",
    "  print(w_at_step)\n",
    "  t = range(0, num_training_steps)\n",
    "  plt.plot(t, loss_at_step, 'k',\n",
    "           t, w_at_step, 'r',\n",
    "           t, [true_w] * num_training_steps, 'r--',\n",
    "           t, b_at_step, 'b',\n",
    "           t, [true_b] * num_training_steps, 'b--')\n",
    "  plt.legend(['loss', 'w estimate', 'w true', 'b estimate', 'b true'])\n",
    "  plt.show()\n",
    "\n",
    "train_model(inputs, labels, wb, optimizer, num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Ways to Compute Gradients\n",
    "\n",
    "Using our loss function as an example (`loss_fn()`), there are several other ways we could compute gradients:\n",
    "\n",
    "1. `tfe.implicit_gradients()`\n",
    "1. `tfe.gradients_function()`\n",
    "1. `tfe.implicit_value_and_gradients()`\n",
    "1. `tfe.value_and_gradients_function()`\n",
    "\n",
    "Each of these functions does the following:\n",
    "* Wraps a function.\n",
    "* Returns a function with the same input signature as the wrapped function.\n",
    "\n",
    "They differ only in what information they return.\n",
    "\n",
    "### Gradients-only functions\n",
    "\n",
    "The following two functions return a function that returns only the variables' gradients:\n",
    "\n",
    "1. `tfe.gradients_function()`: Returns the partial derivatives of the function `f()` with respect to the parameters of `f()`.\n",
    "1. `tfe.implicit_gradients()`: Returns the partial derivatives of the function `f()` with respect to the trainable parameters (`tf.Variable`) used by `f()`.\n",
    "\n",
    "In our example above, the `tf.layers.Dense` object encapsulates the trainable parameters.\n",
    "\n",
    "### Value and gradients functions\n",
    "\n",
    "The following two functions are identical to their counterparts above, except that they also return the value of the wrapped function.\n",
    "\n",
    "1. `tfe.implicit_value_and_gradients()`\n",
    "1. `tfe.value_and_gradients_function()`\n",
    "\n",
    "### Gradient demos\n",
    "\n",
    "In the demos below, we show examples for the `implicit_*` functions, since our existing loss function works seamlessly with these versions. (The other versions require that your parameters are tensors and tensors only; in our example, we're using a `Dense` layer.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfe.implicit_gradients() demo\n",
    "gradients_fn = tfe.implicit_gradients(loss_fn)\n",
    "\n",
    "# Returns only gradients and variables:\n",
    "gradients_fn(inputs, labels, wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfe.implicit_value_and_gradients() demo\n",
    "value_gradients_fn = tfe.implicit_value_and_gradients(loss_fn)\n",
    "\n",
    "# Returns the value returned by the function passed in, gradients, and variables:\n",
    "value_gradients_fn(inputs, labels, wb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
